{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gad_Decision_Tree_Classifier_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBr9MZAw4euX"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class GadId3Classifier:\n",
        "  def fit(self, input, output):\n",
        "    data = input.copy()\n",
        "    data[output.name] = output\n",
        "    self.tree = self.decision_tree(data, data, input.columns, output.name)\n",
        "\n",
        "  def predict(self, input):\n",
        "    # convert input data into a dictionary of samples\n",
        "    samples = input.to_dict(orient='records')\n",
        "    predictions = []\n",
        "\n",
        "    # make a prediction for every sample\n",
        "    for sample in samples:\n",
        "      predictions.append(self.make_prediction(sample, self.tree, 1.0))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "  def entropy(self, attribute_column):\n",
        "    # find unique values and their frequency counts for the given attribute\n",
        "    values, counts = np.unique(attribute_column, return_counts=True)\n",
        "\n",
        "    # calculate entropy for each unique value\n",
        "    entropy_list = []\n",
        "\n",
        "    for i in range(len(values)):\n",
        "      probability = counts[i]/np.sum(counts)\n",
        "      entropy_list.append(-probability*np.log2(probability))\n",
        "\n",
        "    # calculate sum of individual entropy values\n",
        "    total_entropy = np.sum(entropy_list)\n",
        "\n",
        "    return total_entropy\n",
        "  \n",
        "  def information_gain(self, data, feature_attribute_name, target_attribute_name):\n",
        "    # find total entropy of given subset\n",
        "    total_entropy = self.entropy(data[target_attribute_name])\n",
        "\n",
        "    # find unique values and their frequency counts for the attribute to be split\n",
        "    values, counts = np.unique(data[feature_attribute_name], return_counts=True)\n",
        "\n",
        "    # calculate weighted entropy of subset\n",
        "    weighted_entropy_list = []\n",
        "\n",
        "    for i in range(len(values)):\n",
        "      subset_probability = counts[i]/np.sum(counts)\n",
        "      subset_entropy = self.entropy(data.where(data[feature_attribute_name]==values[i]).dropna()[target_attribute_name])\n",
        "      weighted_entropy_list.append(subset_probability*subset_entropy)\n",
        "\n",
        "    total_weighted_entropy = np.sum(weighted_entropy_list)\n",
        "\n",
        "    # calculate information gain\n",
        "    information_gain = total_entropy - total_weighted_entropy\n",
        "\n",
        "    return information_gain\n",
        "\n",
        "  def decision_tree(self, data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class=None):\n",
        "    # base cases:\n",
        "    # if data is pure, return the majority class of subset\n",
        "    unique_classes = np.unique(data[target_attribute_name])\n",
        "    if len(unique_classes) <= 1:\n",
        "      return unique_classes[0]\n",
        "    # if subset is empty, ie. no samples, return majority class of original data\n",
        "    elif len(data) == 0:\n",
        "      majority_class_index = np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])\n",
        "      return np.unique(original_data[target_attribute_name])[majority_class_index]\n",
        "    # if data set contains no features to train with, return parent node class\n",
        "    elif len(feature_attribute_names) == 0:\n",
        "      return parent_node_class\n",
        "    # if none of the above are true, construct a branch:\n",
        "    else:\n",
        "      # determine parent node class of current branch\n",
        "      majority_class_index = np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])\n",
        "      parent_node_class = unique_classes[majority_class_index]\n",
        "\n",
        "      # determine information gain values for each feature\n",
        "      # choose feature which best splits the data, ie. highest value\n",
        "      ig_values = [self.information_gain(data, feature, target_attribute_name) for feature in feature_attribute_names]\n",
        "      best_feature_index = np.argmax(ig_values)\n",
        "      best_feature = feature_attribute_names[best_feature_index]\n",
        "\n",
        "      # create tree structure, empty at first\n",
        "      tree = {best_feature: {}}\n",
        "\n",
        "      # remove best feature from available features, it will become the parent node\n",
        "      feature_attribute_names = [i for i in feature_attribute_names if i != best_feature]\n",
        "\n",
        "      # create nodes under parent node\n",
        "      parent_attribute_values = np.unique(data[best_feature])\n",
        "      for value in parent_attribute_values:\n",
        "        sub_data = data.where(data[best_feature] == value).dropna()\n",
        "\n",
        "        # call the algorithm recursively\n",
        "        subtree = self.decision_tree(sub_data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class)\n",
        "\n",
        "        # add subtree to original tree\n",
        "        tree[best_feature][value] = subtree\n",
        "\n",
        "      return tree\n",
        "\n",
        "  def make_prediction(self, sample, tree, default=1):\n",
        "    # map sample data to tree\n",
        "    for attribute in list(sample.keys()):\n",
        "      # check if feature exists in tree\n",
        "      if attribute in list(tree.keys()):\n",
        "        try:\n",
        "          result = tree[attribute][sample[attribute]]\n",
        "        except:\n",
        "          return default\n",
        "\n",
        "        result = tree[attribute][sample[attribute]]\n",
        "\n",
        "        # if more attributes exist within result, recursively find best result\n",
        "        if isinstance(result, dict):\n",
        "          return self.make_prediction(sample, result)\n",
        "        else:\n",
        "          return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_gpI0czZd_b",
        "outputId": "e3e91879-8609-456f-8960-f40fb945364c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "df = pd.read_csv(data_url, header=None)\n",
        "\n",
        "# rename known columns\n",
        "columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
        "           'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'disease_present']\n",
        "df.columns = columns\n",
        "\n",
        "# convert disease_present feature to binary\n",
        "df['disease_present'] = df.disease_present.replace([1,2,3,4], 1)\n",
        "\n",
        "# drop rows with missing values, missing = ?\n",
        "df = df.replace(\"?\", np.nan)\n",
        "df = df.dropna()\n",
        "\n",
        "# organize data into input and output\n",
        "X = df.drop(columns=\"disease_present\")\n",
        "y = df[\"disease_present\"]\n",
        " \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        " \n",
        "# initialize and fit model\n",
        "model = GadId3Classifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# return accuracy score\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5858585858585859"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T4M-0ObTZhs",
        "outputId": "0049e1e1-8716-45be-f459-3cf3568dbe0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.disease_present.value_counts(normalize=True).max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5387205387205387"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    }
  ]
}